---
title: "PSTAT220A Homework 2"
subtitle: "Due 10/26/2025"
format: 
  pdf:
    geometry:
      - top=30mm
      - left=30mm
---

```{r setup, echo=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(faraway)
library(carData)
```


## Problem 1 (35 pt)

The dataset `Salaries` in library `carData` concerns the salaries for Professors in 2008-2009. Please read the `Salaries` documentation for a full description of the data.

a) Make a numerical and graphical summary of the data, commenting on any features that you find interesting

b) Fit a linear regression model with the salary as the response.  Which variables, if any, are significantly associated with salary?

c) Compute LS estimates in R using the matrix solution to the least squares problem and confirm you get the same estimates as those in (b).  To generate the covariates matrix it may help to use the `model.matrix` function.

d) What percentage of variation in the response is explained by the covariates? Explain whether you use the unadjusted or adjusted measure in your answer and why.

e) Which observation has the largest absolute residual (give the row number)?

f) Report separate 99% confidence intervals for the coefficients associated with `yrs.since.phd` and `yrs.service`.

g) Plot a 95% confidence region for the coefficients associated with `yrs.since.phd` and `yrs.service`. Comment on the resulting shape and why this makes sense.

h) Construct pointwise and simultaneous 95% confidence band for the prediction of future mean response and the prediction of future observations

i) Compute the partial coefficient of determination for `yrs.since.phd`. Interpret the meaning of this quantity.

j) Construct the EHW heteroskedasticity-consistent standard errors for the regression coefficients.  Comment on the comparison between these standard errors to those returned by `lm`. In you response, reference any evidence for (or against) heteroskedasticity.

h) What are the highest leverage and highest influence points?

i) Are the residuals approximately normally distributed? If not suggest a transformation of the outcome that might improve the model ift.



### Part (a): Numerical and Graphical Summary

```{r load-data, echo=TRUE, warning=FALSE, message=FALSE}
library(carData)
library(sandwich)
library(lmtest)
library(ellipse)

data(Salaries)
str(Salaries)
summary(Salaries)
```

Summary statistics by categorical variables:

```{r summary-by-group, echo=TRUE}
aggregate(salary ~ rank, data=Salaries,
          FUN=function(x) c(mean=mean(x), median=median(x), sd=sd(x)))
aggregate(salary ~ discipline, data=Salaries,
          FUN=function(x) c(mean=mean(x), median=median(x), sd=sd(x)))
aggregate(salary ~ sex, data=Salaries,
          FUN=function(x) c(mean=mean(x), median=median(x), sd=sd(x)))
```

```{r graphical-summary, echo=TRUE, fig.width=10, fig.height=8}
par(mfrow=c(2,3))

hist(Salaries$salary, breaks=30, col="lightblue", main="Distribution of Salary",
     xlab="Salary ($)", prob=TRUE)
lines(density(Salaries$salary), col="red", lwd=2)

boxplot(salary ~ rank, data=Salaries, col=c("lightblue","lightgreen","pink"),
        main="Salary by Rank", ylab="Salary ($)", las=2)

boxplot(salary ~ discipline, data=Salaries, col=c("lightblue","lightgreen"),
        main="Salary by Discipline", ylab="Salary ($)")

boxplot(salary ~ sex, data=Salaries, col=c("lightblue","pink"),
        main="Salary by Sex", ylab="Salary ($)")

plot(Salaries$yrs.since.phd, Salaries$salary, col=as.factor(Salaries$rank),
     pch=19, xlab="Years Since PhD", ylab="Salary ($)",
     main="Salary vs Years Since PhD")
legend("topleft", legend=levels(Salaries$rank), col=1:3, pch=19, cex=0.8)

plot(Salaries$yrs.service, Salaries$salary, col=as.factor(Salaries$rank),
     pch=19, xlab="Years of Service", ylab="Salary ($)",
     main="Salary vs Years of Service")
legend("topleft", legend=levels(Salaries$rank), col=1:3, pch=19, cex=0.8)

par(mfrow=c(1,1))
```

**Interesting features:**

1. The salary distribution is right-skewed with some high earners
2. Clear salary differences across ranks: Full Professors earn the most
3. Positive relationship between years since PhD and salary
4. Years since PhD and years of service appear highly correlated (multicollinearity concern)

### Part (b): Linear Regression Model

```{r fit-model, echo=TRUE}
model <- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex,
            data=Salaries)
summary(model)
```

The significantly associated variables (p < 0.05) are: `rank`, `discipline`, `yrs.since.phd`, and `sex`. Notably, `yrs.service` is not significant, likely due to multicollinearity with `yrs.since.phd`.

### Part (c): Matrix Solution to LS

```{r matrix-ls, echo=TRUE}
X <- model.matrix(~ rank + discipline + yrs.since.phd + yrs.service + sex,
                  data=Salaries)
y <- Salaries$salary

beta_hat_matrix <- solve(t(X) %*% X) %*% t(X) %*% y

data.frame(
  lm_estimates = coef(model),
  matrix_estimates = as.vector(beta_hat_matrix),
  difference = coef(model) - as.vector(beta_hat_matrix)
)
```

The estimates match perfectly, confirming the matrix solution.

### Part (d): Percentage of Variation Explained

```{r r-squared, echo=TRUE}
r_squared <- summary(model)$r.squared
adj_r_squared <- summary(model)$adj.r.squared

c(R_squared = r_squared, Adjusted_R_squared = adj_r_squared)
```

We should use the **adjusted R²** (`r round(adj_r_squared, 4)`) because:

1. We have multiple predictor variables
2. Adjusted R² penalizes for model complexity
3. It provides a more honest estimate of predictive performance

**Conclusion:** The model explains approximately `r round(adj_r_squared*100, 2)`% of the variation in salary.

### Part (e): Largest Absolute Residual

```{r max-residual, echo=TRUE}
resids <- residuals(model)
max_resid_idx <- which.max(abs(resids))

max_resid_idx
resids[max_resid_idx]
Salaries[max_resid_idx, ]
```

Observation **`r max_resid_idx`** has the largest absolute residual of **$`r round(resids[max_resid_idx], 2)`**.

### Part (f): 99% Confidence Intervals

```{r ci-99, echo=TRUE}
confint(model, parm=c("yrs.since.phd", "yrs.service"), level=0.99)
```

### Part (g): 95% Confidence Region

```{r confidence-ellipse, echo=TRUE, fig.width=6, fig.height=6}
beta_idx <- which(names(coef(model)) %in% c("yrs.since.phd", "yrs.service"))
beta_est <- coef(model)[beta_idx]
vcov_mat <- vcov(model)[beta_idx, beta_idx]

plot(ellipse(vcov_mat, centre=beta_est, level=0.95), type='l', lwd=2,
     xlab="Coefficient for yrs.since.phd",
     ylab="Coefficient for yrs.service",
     main="95% Confidence Region for Two Coefficients")
points(beta_est[1], beta_est[2], pch=19, col="red", cex=1.5)
abline(h=0, v=0, lty=2, col="gray")
grid()
```

**Shape interpretation:** The ellipse is tilted/elongated, indicating negative correlation between the two coefficient estimates. This makes sense because `yrs.since.phd` and `yrs.service` are highly correlated (multicollinearity). When one coefficient estimate increases, the other tends to compensate in the opposite direction to maintain a similar fit.

### Part (h): Confidence and Prediction Bands

```{r confidence-bands, echo=TRUE, fig.width=8, fig.height=6}
new_data <- data.frame(
  yrs.since.phd = seq(min(Salaries$yrs.since.phd),
                      max(Salaries$yrs.since.phd), length=100),
  yrs.service = median(Salaries$yrs.service),
  rank = "Prof",
  discipline = "A",
  sex = "Male"
)

pred_conf <- predict(model, newdata=new_data, interval="confidence", level=0.95)
pred_pred <- predict(model, newdata=new_data, interval="prediction", level=0.95)

n <- nrow(Salaries)
p <- length(coef(model))
W <- sqrt(p * qf(0.95, p, n-p))
pred_se <- predict(model, newdata=new_data, se.fit=TRUE)$se.fit
simul_conf_lower <- pred_conf[,"fit"] - W * pred_se
simul_conf_upper <- pred_conf[,"fit"] + W * pred_se

plot(new_data$yrs.since.phd, pred_conf[,"fit"], type="l", lwd=2,
     ylim=range(c(pred_pred)),
     xlab="Years since PhD", ylab="Salary ($)",
     main="95% Confidence and Prediction Bands")
lines(new_data$yrs.since.phd, pred_conf[,"lwr"], lty=2, col="blue", lwd=2)
lines(new_data$yrs.since.phd, pred_conf[,"upr"], lty=2, col="blue", lwd=2)
lines(new_data$yrs.since.phd, simul_conf_lower, lty=2, col="darkgreen", lwd=2)
lines(new_data$yrs.since.phd, simul_conf_upper, lty=2, col="darkgreen", lwd=2)
lines(new_data$yrs.since.phd, pred_pred[,"lwr"], lty=2, col="red", lwd=2)
lines(new_data$yrs.since.phd, pred_pred[,"upr"], lty=2, col="red", lwd=2)
legend("topleft",
       legend=c("Fitted", "Pointwise 95% CI", "Simultaneous 95% CI", "95% PI"),
       col=c("black", "blue", "darkgreen", "red"),
       lty=c(1,2,2,2), lwd=2, cex=0.8)
```

The plot shows three types of bands: pointwise confidence intervals (blue), simultaneous confidence bands using Working-Hotelling method (green), and prediction intervals (red). The simultaneous bands are wider to account for multiple comparisons.

### Part (i): Partial Coefficient of Determination

```{r partial-r2, echo=TRUE}
model_reduced <- lm(salary ~ rank + discipline + yrs.service + sex,
                    data=Salaries)

SSR_full <- sum(residuals(model)^2)
SSR_reduced <- sum(residuals(model_reduced)^2)
partial_R2 <- (SSR_reduced - SSR_full) / SSR_reduced

partial_R2
```

**Interpretation:** After controlling for rank, discipline, yrs.service, and sex, `yrs.since.phd` explains an additional `r round(partial_R2*100, 2)`% of the remaining variation in salary. This represents the unique contribution of `yrs.since.phd` beyond what the other variables already explain.

### Part (j): EHW Heteroskedasticity-Consistent Standard Errors

```{r ehw-se, echo=TRUE}
ols_results <- coeftest(model)
ols_results

ehw_results <- coeftest(model, vcov=vcovHC(model, type="HC0"))
ehw_results

se_comparison <- data.frame(
  Parameter = names(coef(model)),
  OLS_SE = ols_results[, "Std. Error"],
  EHW_SE = ehw_results[, "Std. Error"],
  Ratio = ehw_results[, "Std. Error"] / ols_results[, "Std. Error"],
  Difference_Pct = (ehw_results[, "Std. Error"] - ols_results[, "Std. Error"]) /
                    ols_results[, "Std. Error"] * 100
)
se_comparison
```

```{r heteroskedasticity-tests, echo=TRUE, fig.width=10, fig.height=8}
bp_test <- bptest(model)
bp_test

par(mfrow=c(2,2))

plot(fitted(model), residuals(model),
     xlab="Fitted values", ylab="Residuals",
     main="Residuals vs Fitted Values", pch=19, col=rgb(0,0,1,0.5))
abline(h=0, lty=2, col="red", lwd=2)
lines(lowess(fitted(model), residuals(model)), col="darkred", lwd=2)

plot(fitted(model), sqrt(abs(rstandard(model))),
     xlab="Fitted values", ylab=expression(sqrt("|Standardized residuals|")),
     main="Scale-Location Plot", pch=19, col=rgb(0,0,1,0.5))
lines(lowess(fitted(model), sqrt(abs(rstandard(model)))), col="darkred", lwd=2)

plot(Salaries$yrs.since.phd, residuals(model),
     xlab="Years since PhD", ylab="Residuals",
     main="Residuals vs Years since PhD", pch=19, col=rgb(0,0,1,0.5))
abline(h=0, lty=2, col="red", lwd=2)
lines(lowess(Salaries$yrs.since.phd, residuals(model)), col="darkred", lwd=2)

boxplot(residuals(model) ~ Salaries$rank,
        xlab="Rank", ylab="Residuals",
        main="Residuals by Rank",
        col=c("lightblue","lightgreen","pink"))
abline(h=0, lty=2, col="red", lwd=2)

par(mfrow=c(1,1))
```

**Evidence for/against Heteroskedasticity:**

- **Breusch–Pagan test:** p = `r signif(bp_test$p.value, 3)` — `r if (bp_test$p.value < 0.05) "reject homoskedasticity (evidence of heteroskedasticity)" else "fail to reject homoskedasticity"`.
- **Residual plots:** Residuals vs Fitted shows changing spread (fan-out) with fitted values; Scale–Location trend is non-flat.
- **SE comparison:** EHW robust SEs are generally larger than OLS (avg diff `r round(mean(abs(se_comparison$Difference_Pct)), 1)`%).

**Conclusion:** `r if (bp_test$p.value < 0.05) "There is evidence of heteroskedasticity; report EHW robust standard errors for inference." else "No strong evidence of heteroskedasticity; OLS standard errors are adequate."`

### Part (k): Leverage and Influence

```{r leverage-influence, echo=TRUE, fig.width=8, fig.height=4}
h <- hatvalues(model)
high_lev <- which.max(h)

cooks <- cooks.distance(model)
high_cook <- which.max(cooks)

# Display results
cat("Highest leverage point: Observation", high_lev, "with h =", round(h[high_lev], 4), "\n")
cat("Highest influence point: Observation", high_cook, "with Cook's D =", round(cooks[high_cook], 4), "\n")

# Simple diagnostic plot
par(mfrow=c(1,2))
plot(h, ylab="Leverage", main="Leverage Values", pch=19, col="blue")
points(high_lev, h[high_lev], col="red", pch=19, cex=2)

plot(cooks, ylab="Cook's Distance", main="Influence", pch=19, col="blue")
points(high_cook, cooks[high_cook], col="red", pch=19, cex=2)
par(mfrow=c(1,1))
```

### Part (l): Residual Normality

```{r normality-check, echo=TRUE, fig.width=10, fig.height=5}
par(mfrow=c(1,2))
qqnorm(residuals(model))
qqline(residuals(model), col="red", lwd=2)

hist(residuals(model), breaks=30, prob=TRUE, col="lightblue",
     main="Histogram of Residuals", xlab="Residuals")
curve(dnorm(x, mean=mean(residuals(model)), sd=sd(residuals(model))),
      add=TRUE, col="red", lwd=2)
par(mfrow=c(1,1))

sw_test <- shapiro.test(residuals(model))
sw_test
```

The Shapiro-Wilk test has p-value = `r round(sw_test$p.value, 4)`. If p < 0.05, the residuals are not normally distributed.

```{r log-transformation, echo=TRUE, eval=sw_test$p.value < 0.05, fig.width=10, fig.height=5}
# Try log transformation if needed
model_log <- lm(log(salary) ~ rank + discipline + yrs.since.phd +
                yrs.service + sex, data=Salaries)

par(mfrow=c(1,2))
qqnorm(residuals(model_log), main="QQ Plot - log(salary)")
qqline(residuals(model_log), col="red", lwd=2)

hist(residuals(model_log), breaks=30, prob=TRUE, col="lightgreen",
     main="Residuals - log(salary)", xlab="Residuals")
curve(dnorm(x, mean=mean(residuals(model_log)), sd=sd(residuals(model_log))),
      add=TRUE, col="red", lwd=2)
par(mfrow=c(1,1))

shapiro.test(residuals(model_log))
```

**Suggested transformation:** If normality is violated (right skewness), try `log(salary)` transformation to improve model fit.


## Problem 2 (25 pt)

A series of $n + 1$ observations $y_i$ ($i = 1, \ldots, n + 1$) are taken from a normal distribution with unknown variance $\sigma^2$. After the first $n$ observations it is suspected that there is a sudden change in the mean of the distribution. That is, assume the first $n$ observations are iid   $y_1, ..., y_n \sim N(\mu, \sigma^2)$  the $y_{(n + 1)} \sim N(\mu + \delta, \sigma^2)$.

a) Write this model in the matrix form $y = X\beta + \epsilon$

b) Derive the LS estimates of $\mu$ and $\delta$

c) Derive a test statistic for testing the hypothesis that the $(n + 1)$st observation has the same population mean as the previous observations, that is, the two mean parameters are equal.

d) Assume that $\sigma^2=1$ and $\delta=2$.  Simulate the distribution of the test statistic under this alternative hypothesis and compute the power of the test to detect $\delta  \neq 0$ by counting the fraction of times the test statistic rejects.  Assume you design your test with Type I error of 5% and are conducting a 2-sided test.

### Part (a): Matrix Form

The model can be written as $y = X\beta + \epsilon$ where:

$$
y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \\ y_{n+1} \end{pmatrix}, \quad
X = \begin{pmatrix} 1 & 0 \\ 1 & 0 \\ \vdots & \vdots \\ 1 & 0 \\ 1 & 1 \end{pmatrix}, \quad
\beta = \begin{pmatrix} \mu \\ \delta \end{pmatrix}, \quad
\epsilon = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \\ \epsilon_{n+1} \end{pmatrix}
$$

where $\epsilon_i \sim N(0, \sigma^2)$ independently.

The first column of $X$ corresponds to the intercept $\mu$, and the second column is an indicator for the $(n+1)$st observation, corresponding to the shift $\delta$.

### Part (b): LS Estimates

The least squares estimates are given by:
$$\hat{\beta} = (X^TX)^{-1}X^Ty$$

First, compute $X^TX$:
$$X^TX = \begin{pmatrix} n+1 & 1 \\ 1 & 1 \end{pmatrix}$$

The inverse is:
$$\begin{pmatrix} n+1 & 1 \\ 1 & 1 \end{pmatrix}^{-1} = \frac{1}{n} \begin{pmatrix} 1 & -1 \\ -1 & n+1 \end{pmatrix}$$

Next, compute $X^Ty$:
$$X^Ty = \begin{pmatrix} \sum_{i=1}^{n+1} y_i \\ y_{n+1} \end{pmatrix}$$

Therefore:
$$\hat{\beta} = \frac{1}{n} \begin{pmatrix} 1 & -1 \\ -1 & n+1 \end{pmatrix} \begin{pmatrix} \sum_{i=1}^{n+1} y_i \\ y_{n+1} \end{pmatrix} = \frac{1}{n} \begin{pmatrix} \sum_{i=1}^{n+1} y_i - y_{n+1} \\ -\sum_{i=1}^{n+1} y_i + (n+1)y_{n+1} \end{pmatrix}$$

Simplifying:
$$\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n} y_i = \bar{y}_n$$

$$\hat{\delta} = \frac{1}{n}\left(-\sum_{i=1}^{n} y_i - y_{n+1} + (n+1)y_{n+1}\right) = \frac{1}{n}\left(ny_{n+1} - \sum_{i=1}^{n} y_i\right) = y_{n+1} - \bar{y}_n$$

### Part (c): Test Statistic

We want to test $H_0: \delta = 0$ vs $H_1: \delta \neq 0$.

Under $H_0$, the test statistic is:
$$t = \frac{\hat{\delta}}{\hat{\sigma}\sqrt{(X^TX)^{-1}_{22}}}$$

where $\hat{\sigma}^2 = \frac{1}{n-1}\sum_{i=1}^{n+1}(y_i - \hat{y}_i)^2$ is the residual variance.

From part (b), $(X^TX)^{-1}_{22} = \frac{n+1}{n}$.

Therefore:
$$t = \frac{y_{n+1} - \bar{y}_n}{\hat{\sigma}\sqrt{\frac{n+1}{n}}}$$

Under $H_0$, $t \sim t_{n-1}$ (with $n-1$ degrees of freedom, since we have $n+1$ observations and 2 parameters).

### Part (d): Power Simulation

```{r problem2-simulation, echo=TRUE, fig.width=8, fig.height=6}
set.seed(220)

# Simulation parameters
n <- 30
sigma_true <- 1
delta_true <- 2
nsim <- 10000
alpha <- 0.05

# Critical value for two-sided test under H0
critical_value <- qt(1 - alpha/2, df = n - 1)

# Store test statistics and rejection decisions
t_stats <- numeric(nsim)
rejections <- logical(nsim)

for (i in 1:nsim) {
  # Generate data under alternative hypothesis
  y <- c(rnorm(n, mean = 0, sd = sigma_true),
         rnorm(1, mean = delta_true, sd = sigma_true))

  # Compute estimates
  y_bar_n <- mean(y[1:n])
  delta_hat <- y[n+1] - y_bar_n

  # Compute residual standard error
  # Fitted values
  y_fitted <- c(rep(y_bar_n, n), y_bar_n + delta_hat)
  residuals <- y - y_fitted
  sigma_hat <- sqrt(sum(residuals^2) / (n - 1))

  # Compute test statistic
  se_delta <- sigma_hat * sqrt((n+1)/n)
  t_stats[i] <- delta_hat / se_delta

  # Check if we reject H0
  rejections[i] <- abs(t_stats[i]) > critical_value
}

# Compute power
power <- mean(rejections)

# Plot distribution of test statistics
hist(t_stats, breaks=50, prob=TRUE, col="lightblue",
     main="Distribution of Test Statistic under H1",
     xlab="t-statistic", ylim=c(0, 0.4))
curve(dt(x, df = n-1), add=TRUE, col="red", lwd=2, lty=2)
abline(v=c(-critical_value, critical_value), col="darkred", lwd=2, lty=2)
legend("topright",
       legend=c("Simulated", "t-distribution under H0", "Critical values"),
       col=c("lightblue", "red", "darkred"),
       lty=c(1, 2, 2), lwd=c(10, 2, 2))
```

**Results:**

- Critical value (two-sided, α = 0.05): ±`r round(critical_value, 3)`
- Power of the test: `r round(power, 4)` (`r round(power*100, 2)`%)
- Number of rejections: `r sum(rejections)` out of `r nsim`

**Interpretation:** The power of `r round(power*100, 2)`% indicates that when $\delta = 2$ and $\sigma^2 = 1$ with $n = 30$, we correctly reject the null hypothesis (detect the mean shift) about `r round(power*100, 2)`% of the time. This is relatively high power, suggesting the test is effective at detecting this magnitude of shift.

## Problem 3 (25 pt)

In this problem we'll conduct a simulation to confirm and explore some important theoretical results. 

a) Use simulation to confirm that $\frac{\hat{\beta}_i - \beta_i}{\hat{\sigma}\sqrt{(X^T X)^{-1}_{ii}}} \sim t_{n-p}$.

b) Compute the coverage of the associated confidence intervals for parameters. Do they have the desired coverage?

c) Evaluate the performance of hypothesis tests of parameters. Discuss type I errors and powers.

d) Repeat a)-c) assuming that the Gaussianity assumption is violated by generating non-Gaussian random error.  Run one set of simulations with symmetric but heavy-tailed residual distribution and another with a skewed residual distribution. How did these violations influence your results? Which violation appeared worse (heavy-tailed or skewed)?

### Part (a): Verify t-distribution

We simulate data from a linear regression model and verify that the standardized coefficient estimates follow a t-distribution.

```{r problem3-setup, echo=TRUE}
set.seed(220)

n <- 50         
p <- 4           
nsim <- 10000    
beta_true <- c(2, -1, 3, 0.5)  

simulate_regression <- function(n, beta_true, error_dist = "normal") {
  p <- length(beta_true)

  X <- cbind(1, matrix(rnorm(n * (p-1)), n, p-1))

  if (error_dist == "normal") {
    epsilon <- rnorm(n, mean = 0, sd = 2)
  } else if (error_dist == "heavy_tailed") {
    epsilon <- rt(n, df = 3) * 2  
  } else if (error_dist == "skewed") {
    epsilon <- (rchisq(n, df = 2) - 2) * sqrt(2)  
  }

  y <- X %*% beta_true + epsilon

  fit <- lm(y ~ X - 1)

  beta_hat <- coef(fit)
  se_beta <- summary(fit)$coefficients[, "Std. Error"]

  t_stats <- (beta_hat - beta_true) / se_beta

  ci <- confint(fit, level = 0.95)
  coverage <- (ci[,1] <= beta_true) & (beta_true <= ci[,2])

  p_values <- 2 * pt(-abs(t_stats), df = n - p)

  return(list(t_stats = t_stats, coverage = coverage, p_values = p_values,
              beta_hat = beta_hat))
}
```

```{r problem3a-normal, echo=TRUE, fig.width=10, fig.height=8}
results_normal <- replicate(nsim, simulate_regression(n, beta_true, "normal"),
                            simplify = FALSE)

t_stats_matrix <- sapply(results_normal, function(x) x$t_stats)

par(mfrow = c(2, 2))
for (j in 1:p) {
  hist(t_stats_matrix[j, ], breaks = 50, prob = TRUE, col = "lightblue",
       main = paste0("t-statistic for β", j-1),
       xlab = "t-statistic", ylim = c(0, 0.45))
  curve(dt(x, df = n - p), add = TRUE, col = "red", lwd = 2)

  qqnorm(t_stats_matrix[j, ], main = paste0("QQ Plot for β", j-1))
  qqline(t_stats_matrix[j, ], col = "red", lwd = 2)
}
par(mfrow = c(1, 1))
```

### Part (b): Confidence Interval Coverage

```{r problem3b, echo=TRUE}
coverage_matrix <- sapply(results_normal, function(x) x$coverage)
coverage_rates <- rowMeans(coverage_matrix)

coverage_df <- data.frame(
  Parameter = paste0("β", 0:(p-1)),
  True_Value = beta_true,
  Coverage_Rate = coverage_rates,
  Target = 0.95
)
coverage_df
```

**Analysis:** The coverage rates for all parameters are very close to the nominal 95% level (within `r round(max(abs(coverage_rates - 0.95))*100, 2)`%), confirming that the confidence intervals have the desired coverage under normal errors.

### Part (c): Hypothesis Test Performance

We test two scenarios:
1. **Type I error:** Test $H_0: \beta_j = \beta_{\text{true},j}$ (should reject 5% of the time)
2. **Power:** Test $H_0: \beta_j = 0$ when $\beta_{\text{true},j} \neq 0$

```{r problem3c, echo=TRUE}
p_values_matrix <- sapply(results_normal, function(x) x$p_values)
type1_errors <- rowMeans(p_values_matrix < 0.05)

power_results <- replicate(nsim, {
  X <- cbind(1, matrix(rnorm(n * (p-1)), n, p-1))
  epsilon <- rnorm(n, mean = 0, sd = 2)
  y <- X %*% beta_true + epsilon

  fit <- lm(y ~ X - 1)

  coef_summary <- summary(fit)$coefficients
  p_vals <- coef_summary[, "Pr(>|t|)"]

  return(p_vals)
}, simplify = TRUE)

power_rates <- rowMeans(power_results < 0.05)

test_performance <- data.frame(
  Parameter = paste0("β", 0:(p-1)),
  True_Value = beta_true,
  Type_I_Error = type1_errors,
  Power_vs_Zero = power_rates
)
test_performance
```

**Analysis:**

- **Type I errors:** All close to 5%, as expected under the null hypothesis
- **Power:** Higher for parameters with larger true values (e.g., $\beta_0 = 2$, $\beta_2 = 3$) and lower for smaller values (e.g., $\beta_3 = 0.5$)

### Part (d): Non-Gaussian Errors

Now we repeat the analysis with non-normal errors: heavy-tailed (t-distribution) and skewed (chi-square).

```{r problem3d-heavy, echo=TRUE, fig.width=10, fig.height=8}
results_heavy <- replicate(nsim, simulate_regression(n, beta_true, "heavy_tailed"),
                           simplify = FALSE)

t_stats_heavy <- sapply(results_heavy, function(x) x$t_stats)
coverage_heavy <- rowMeans(sapply(results_heavy, function(x) x$coverage))
p_values_heavy <- sapply(results_heavy, function(x) x$p_values)
type1_heavy <- rowMeans(p_values_heavy < 0.05)

par(mfrow = c(1, 2))
hist(t_stats_matrix[2, ], breaks = 50, prob = TRUE, col = "lightblue",
     main = "Normal Errors: β1",
     xlab = "t-statistic", xlim = c(-5, 5), ylim = c(0, 0.45))
curve(dt(x, df = n - p), add = TRUE, col = "red", lwd = 2)

hist(t_stats_heavy[2, ], breaks = 50, prob = TRUE, col = "lightcoral",
     main = "Heavy-Tailed Errors: β1",
     xlab = "t-statistic", xlim = c(-5, 5), ylim = c(0, 0.45))
curve(dt(x, df = n - p), add = TRUE, col = "red", lwd = 2)
par(mfrow = c(1, 1))
```

```{r problem3d-skewed, echo=TRUE, fig.width=10, fig.height=8}
results_skewed <- replicate(nsim, simulate_regression(n, beta_true, "skewed"),
                            simplify = FALSE)

t_stats_skewed <- sapply(results_skewed, function(x) x$t_stats)
coverage_skewed <- rowMeans(sapply(results_skewed, function(x) x$coverage))
p_values_skewed <- sapply(results_skewed, function(x) x$p_values)
type1_skewed <- rowMeans(p_values_skewed < 0.05)

par(mfrow = c(1, 2))
hist(t_stats_matrix[2, ], breaks = 50, prob = TRUE, col = "lightblue",
     main = "Normal Errors: β1",
     xlab = "t-statistic", xlim = c(-5, 5), ylim = c(0, 0.45))
curve(dt(x, df = n - p), add = TRUE, col = "red", lwd = 2)

hist(t_stats_skewed[2, ], breaks = 50, prob = TRUE, col = "lightgreen",
     main = "Skewed Errors: β1",
     xlab = "t-statistic", xlim = c(-5, 5), ylim = c(0, 0.45))
curve(dt(x, df = n - p), add = TRUE, col = "red", lwd = 2)
par(mfrow = c(1, 1))
```

```{r problem3d-comparison, echo=TRUE}
comparison_df <- data.frame(
  Parameter = paste0("β", 0:(p-1)),
  Coverage_Normal = coverage_rates,
  Coverage_Heavy = coverage_heavy,
  Coverage_Skewed = coverage_skewed,
  Type1_Normal = type1_errors,
  Type1_Heavy = type1_heavy,
  Type1_Skewed = type1_skewed
)
comparison_df
```

**Summary:** Heavy-tailed errors (symmetric) show minimal impact on coverage and Type I error rates, while skewed errors cause larger deviations from nominal levels. **Conclusion:** Skewness is more problematic than heavy tails for regression inference.

## Problem 4 (15 pts)

This problem concerns the `divusa` data in the `faraway` library.

a) Make a well-constructed visualization showing how divorce rate is changing over time.  Does it appear to be steady, going up, or going down?

b) Fit a regression model with divorce as the response and remaining variables as covariates. Interpret the coefficient on `year` (include units).  How can you reconcile this result with the answer to the previous part?

c)  Why might observations be correlated? Make two graphical checks for correlated errors. What do you conclude?

d) Conduct a statistical test the presence of autocorrelation.

### Part (a): Visualization of Divorce Rate Over Time

```{r problem4-load, echo=TRUE}
library(faraway)
data(divusa)

str(divusa)
head(divusa)
```

```{r problem4a-plot, echo=TRUE, fig.width=10, fig.height=6}
plot(divusa$year, divusa$divorce, type = "b", pch = 19, col = "darkblue",
     xlab = "Year", ylab = "Divorce Rate (per 1000)",
     main = "U.S. Divorce Rate Over Time",
     lwd = 2)

lines(lowess(divusa$year, divusa$divorce, f = 0.3), col = "red", lwd = 2, lty = 2)

grid()

legend("topleft",
       legend = c("Observed", "Lowess Smoother"),
       col = c("darkblue", "red"),
       lty = c(1, 2), pch = c(19, NA), lwd = 2)
```

**Observation:** The divorce rate appears to show an overall **increasing trend** from the 1950s, peaking around the late 1970s to early 1980s, followed by a **decline** in more recent years. The pattern is not linear - it's more of an inverted U-shape with considerable year-to-year variation.

### Part (b): Regression Model

```{r problem4b-model, echo=TRUE}
model_div <- lm(divorce ~ year + unemployed + femlab + marriage + birth + military,
                data = divusa)
summary(model_div)
```

**Interpretation:** The `year` coefficient is **`r round(coef(model_div)["year"], 4)`** per 1000 population per year (or **`r round(coef(model_div)["year"] * 10, 3)`** per 1000 over 10 years), holding other variables constant.

**Reconciliation with Part (a):** The linear coefficient may differ from the visual inverted-U pattern due to confounding variables (unemployment, female labor force, marriage/birth rates) and the model's inability to capture non-linear trends.

### Part (c): Checking for Correlated Errors

**Why might observations be correlated?** Time series data exhibit autocorrelation due to temporal persistence of social trends and slowly-changing omitted variables (cultural attitudes, economic conditions).

```{r problem4c-checks, echo=TRUE, fig.width=10, fig.height=8}
resids <- residuals(model_div)

par(mfrow = c(2, 2))

plot(divusa$year, resids, type = "b", pch = 19,
     xlab = "Year", ylab = "Residuals",
     main = "Residuals vs. Time")
abline(h = 0, col = "red", lty = 2, lwd = 2)
lines(lowess(divusa$year, resids), col = "blue", lwd = 2)

plot(resids[-length(resids)], resids[-1],
     pch = 19, col = "darkgreen",
     xlab = "Residual at time t", ylab = "Residual at time t+1",
     main = "Lag-1 Plot of Residuals")
abline(h = 0, v = 0, col = "gray", lty = 2)
abline(lm(resids[-1] ~ resids[-length(resids)]), col = "red", lwd = 2)

acf(resids, main = "Autocorrelation Function of Residuals", lag.max = 20)

pacf(resids, main = "Partial Autocorrelation Function", lag.max = 20)

par(mfrow = c(1, 1))
```

**Conclusions:**

1. **Residuals vs. Time:** Shows clear patterns and runs of positive/negative residuals, suggesting temporal correlation

2. **Lag-1 Plot:** Positive slope indicates that consecutive residuals are correlated - when one residual is positive, the next tends to be positive as well

3. **ACF Plot:** Multiple significant autocorrelations (beyond the dashed blue lines) at various lags, confirming substantial serial correlation

4. **PACF Plot:** Helps identify the order of autoregressive structure

**Overall:** Strong evidence of correlated errors, violating the independence assumption of OLS regression.

### Part (d): Statistical Test for Autocorrelation

```{r problem4d-test, echo=TRUE}
library(lmtest)
dw_test <- dwtest(model_div)
dw_test

bg_test <- bgtest(model_div, order = 3)
bg_test

box_test <- Box.test(resids, lag = 10, type = "Ljung-Box")
box_test
```

**Test Results:**

- **Durbin-Watson:** DW = `r round(dw_test$statistic, 4)`, p = `r round(dw_test$p.value, 4)` — `r ifelse(dw_test$p.value < 0.05, "significant autocorrelation", "no autocorrelation")`
- **Breusch-Godfrey:** LM = `r round(bg_test$statistic, 4)`, p = `r round(bg_test$p.value, 4)` — `r ifelse(bg_test$p.value < 0.05, "significant autocorrelation", "no autocorrelation")`
- **Box-Ljung:** χ² = `r round(box_test$statistic, 4)`, p = `r round(box_test$p.value, 4)` — `r ifelse(box_test$p.value < 0.05, "significant autocorrelation", "no autocorrelation")`

**Conclusion:** `r ifelse(dw_test$p.value < 0.05 | bg_test$p.value < 0.05 | box_test$p.value < 0.05, "Strong evidence of autocorrelation; OLS standard errors are underestimated and inference is unreliable. Consider time series methods (GLS, ARIMA).", "No strong evidence of autocorrelation; OLS inference is adequate.")`
